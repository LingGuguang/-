你吃了吗？
您吃了吗？
您翅吃了吗？
吃了吗您？
吃了吗？

EDA： Easy Data Augmentation简单容易的数据增强----易数据增强。
看上面询问是否吃饭了的问句，可以直观地发现，对句子进行一定规则的修改不会影响句子的意思。因此产生了一系列简单的数据增强方法，统称易数据增强。其中包括：
1.同义词替换：各种NLP处理包都可以做同义词查询（你->您）， 这都归功于word embedding。
2.随机插入：北儿京儿人儿和天儿津儿人儿喜欢把“吃”发成“翅”音， 因此语音识别可能出错。所以把“翅”字随机插入原句里，有利于抗这种干扰。
3.随机交换：一般都会说“您吃了吗”，但说“吃了吗您”的也不在少数，所以对句子进行单词的交换就有必要了。
4.随机删除：有时候说话会省略一些单词， 因此随机删除也是必要的。
*5.回译：把句子从汉语翻译成英语再翻译回汉语，可能获得句子结构上的改变，但语义八九不离十，所以也可以作为增强数据的强力手段。典型例子就是谷歌生草机。

pairing samples: 
这是对图片的数据增强方法。
从训练集中抽两个图片AB， 都有标签。把AB的图像像素加和取平均， 但只使用图片A的标签。这其实是一种对原数据的噪声扰动，加和平均后的图一定包含两个种类图片的特征。个人觉得扰动有点大， 好歹取个64开吧。

mixup：
记得是李宏毅的论文，也是图片数据增强，是上一个方法的更随机的混合。
首先请去查看beta(x,y)分布的图像。
随机一个u~beta(x,x)，0<=u<=1，取(x,x)是为了beta分布两侧对称。 
然后随便抽两个图A和B，对应标签向量为a和b.
将A和B以u:(1-u)的比例加和， 标签向量也是如此。
新混合出来的图片和标签，就是数据增强的结果。
当然啦，用不用beta分布都可以，就看你想混合到什么程度。如果你不介意pairing samples的混合方法， 那么直接用均匀分布代替beta分布就可以了。

mixmatch: 
这是一种图像数据增强的半监督方法。
原文名称：MixMatch: A Holistic Approach to Semi-Supervised Learning
19年5月份的论文。
↓↓↓---下面阐述方法---↓↓↓
你有一批已经标注的数据A。
还有一批没有标注的数据B。
还得有个训练了差不都的分类器。
AB包含的数据量一样。这是硬性规定。
首先，将B进行图像的简单增强K次（反转，模糊等一系列常规操作）。
然后把K张图片都放到分类器里预测，将K个预测的值取加和平均。
然后经过一个叫做“sharpen”的操作， 让预测概率高的概率更高。具体用什么公式做“sharpen”请看论文。
此时， 数据集A有自己的标签， 数据集B有sharpen之后的标签。
将A和B混合打乱成一个大的混合数据集W。
取前一半与A做mixup，形成一个新数据集A`。
取后一半与B做mixup，形成一个新数据集B`。
此时，A`的标签含金量应该很高，B`的含金量虽然不高，但mixup后总会靠点谱。
所以针对A`和B`的状况，再训练时分别使用不同的loss。A`用交叉熵，B`用二范数的平方，整体loss还有一个超参数控制B`的loss。具体请一定看论文，文字说不明白。

顺便推荐一个20年7月的论文， 探讨了小数据+增强能达到的水平，结果很让人惊讶。
论文名称：Can We Achieve More with Less?
链接：https://arxiv.org/pdf/2007.00875.pdf
作者使用kaggle的比赛Toxic Comment Classification的数据集（来自wikipedia toxic comments dataset）探讨了EDA和回译对LR，SVM，Bi-LSTM的作用。
从训练数据中抽取5%的数据加上数据增广，再用三个模型来训练。最后发现，EDA适用于词袋模型，回译适用于语义模型。<---就这句话最重要

