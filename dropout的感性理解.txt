理解dropout，要先理解过拟合是怎么产生的。

一、过拟合
训练集准确率很高，测试集却很差。这是过拟合的特征。
良好的训练集应该能良好地表示客观的数据分布。但限于噪音和财力，数据必带一些极端奇葩的特例，这些特例是模型不该学习的。
但随着神经网络越来越复杂，越来越大，网络就有能力把自己扭曲到极限，以将那些奇葩特例学习正确。模型学坏了。

二、dropout
既然你学坏了，我就棍棒底下出孝子。
模型的复杂势必导致模型中每个神经元都对整体起着很大的作用，少了哪个都会让结果大相径庭。那我就随机dropout一些，这样本来过拟合效果极好(指loss趋于0)，现在却烂的出奇(指loss很高)，模型就会自动地降低模型复杂度，以寻求差不多的效果(指loss比较小)。

三、应该这样吗？
解释过拟合时说了，模型能过拟合是因为他有能力做到过拟合，但如果是个很难的任务，现有模型的学习能力不够，那就谈不上过拟合。
反过来说，降低模型学习能力亦是一种避免过拟合的方法。而学习能力和参数量成正比。
同理，不改变学习能力，单纯地提升任务的难度，也是避免过拟合的方法。总之要努利让模型相对任务变笨。

这一点在ALBERT上有很好的体现。